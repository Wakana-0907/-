{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61004f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'Python 3.12.11' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31m'ipykernel' を Python 環境にインストールします。\n",
      "\u001b[1;31mコマンド: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# webスクレイピングに最低限必要なライブラリをインポート\n",
    "import requests\n",
    "from bs4 import BeautifulSoup     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576ad81",
   "metadata": {},
   "source": [
    "## HTTPリクエスト"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518958be",
   "metadata": {},
   "source": [
    "### 基本のリクエスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4863fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# アクセスしたいwebページのURLを指定\n",
    "url = \"https://www.musashino-u.ac.jp/\"\n",
    "\n",
    "# webサーバーにHTTPリクエストを送信\n",
    "# レスポンスを変数に格納しておく\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95046386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レスポンス\n",
    "\n",
    "print(f\"レスポンス：{res}\") # レスポンスオブジェクトを表示\n",
    "print(f\"レスポンスの型：{type(res)}\") # レスポンスオブジェクトの型を表示\n",
    "print(f\"ステータスコード：{res.status_code}\") # ステータスコードを表示\n",
    "print(f\"ステータスメッセージ：{res.reason}\") # ステータスメッセージを表示\n",
    "\n",
    "print(f\"リクエスト:{res.request}\") # リクエストオブジェクトを表示\n",
    "print(f\"リクエストの型：{type(res.request)}\") # リクエストオブジェクトの型を表示\n",
    "\n",
    "print(f\"リクエストヘッダー：{res.request.headers}\") # リクエストヘッダーを表示\n",
    "print(f\"リクエスト URL：{res.request.url}\") # リクエストURLを表示\n",
    "print(f\"リクエストメソッド：{res.request.method}\") # リクエストメソッドを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487facf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レスポンス\n",
    "\n",
    "print(f\"レスポンスヘッダー：{res.headers}\") # レスポンスヘッダーを表示\n",
    "print(f\"レスポンスボディ：{res.text}\") # レスポンスボディを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# アクセスしたいwebページのURLを指定\n",
    "url = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75213bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for文等で特定のwebサイトに連続でアクセスする場合は、サーバーに負担をかけないように適切な感覚をあけること\n",
    "for i in range(5):\n",
    "    time.sleep(1) # プログラミを1秒間停止\n",
    "    print(f\"アクセス回数: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71928e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習問題 下記のページから出演作品のタイトルを取得する.\n",
    "https://abehiroshi.la.coocan.jp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d653f0",
   "metadata": {},
   "source": [
    "## 最終課題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import json # 辞書を見やすく表示するためにjsonモジュールを追加\n",
    "\n",
    "# --- 設定 ---\n",
    "# 武蔵野大学のトップページURLを設定\n",
    "START_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "\n",
    "# 収集対象とするドメインを設定\n",
    "DOMAIN = urlparse(START_URL).netloc\n",
    "\n",
    "# 収集済みのURLを格納するセット（重複防止用）\n",
    "visited_urls = set()\n",
    "\n",
    "# 結果を格納する辞書型変数\n",
    "page_data = {}\n",
    "# ---\n",
    "\n",
    "def get_page_info(url):\n",
    "    \"\"\"\n",
    "    指定されたURLからHTMLを取得し、<title>と同一ドメイン内のリンクを抽出する\n",
    "    \"\"\"\n",
    "    # URLにクエリやフラグメントがある場合は除去し、正規化\n",
    "    parsed_url = urlparse(url)\n",
    "    clean_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path\n",
    "    \n",
    "    if clean_url in visited_urls:\n",
    "        return []\n",
    "    \n",
    "    # 既に訪問済みとして登録\n",
    "    visited_urls.add(clean_url)\n",
    "    \n",
    "    # URLにアクセス\n",
    "    try:\n",
    "        # サーバーに負荷をかけすぎないよう、適度に遅延を入れる\n",
    "        time.sleep(0.5) \n",
    "        \n",
    "        # タイムアウトを設定\n",
    "        response = requests.get(clean_url, timeout=10)\n",
    "        \n",
    "        # 成功ステータス (200) 以外はスキップ\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Skipping {clean_url} due to status code {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "        # HTMLコンテンツをBeautifulSoupで解析\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 1. <title>を取得\n",
    "        title_tag = soup.find('title')\n",
    "        title_text = title_tag.text.strip() if title_tag else \"No Title Found\"\n",
    "        \n",
    "        # 辞書型変数に格納\n",
    "        page_data[clean_url] = title_text\n",
    "        print(f\"Collected: URL: {clean_url}, Title: {title_text}\")\n",
    "        \n",
    "        # 2. 同一ドメインの全てのリンクを抽出\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag.get('href')\n",
    "            # 絶対URLに変換 (相対URLも処理するため)\n",
    "            absolute_url = urljoin(clean_url, href)\n",
    "            \n",
    "            # クエリパラメータやフラグメントを除去し、正規化\n",
    "            parsed_link = urlparse(absolute_url)\n",
    "            clean_link = parsed_link.scheme + \"://\" + parsed_link.netloc + parsed_link.path\n",
    "            \n",
    "            # ドメインが一致し、まだ訪問していないURLのみをリストに追加\n",
    "            if parsed_link.netloc == DOMAIN and clean_link not in visited_urls:\n",
    "                links.append(clean_link)\n",
    "\n",
    "        return links\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # アクセスエラー（タイムアウト、接続拒否など）が発生した場合\n",
    "        print(f\"Error accessing {clean_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def crawl(start_url):\n",
    "    \"\"\"\n",
    "    同一ドメイン内のリンクを再帰的に辿るメイン関数 (幅優先探索)\n",
    "    \"\"\"\n",
    "    # 待機中のURLリスト (キューとして使用)\n",
    "    queue = [start_url]\n",
    "    \n",
    "    # キューが空になるまでループ\n",
    "    while queue:\n",
    "        current_url = queue.pop(0) # キューの先頭からURLを取り出す\n",
    "        \n",
    "        # ページの情報を取得し、新しいリンク（同一ドメイン内かつ未訪問）を取得\n",
    "        new_links = get_page_info(current_url)\n",
    "        \n",
    "        # 新しいリンクをキューに追加\n",
    "        for link in new_links:\n",
    "            if link not in visited_urls and link not in queue:\n",
    "                queue.append(link)\n",
    "\n",
    "# --- 実行 ---\n",
    "print(f\"Starting crawl on: {START_URL}\")\n",
    "crawl(START_URL)\n",
    "\n",
    "# 辞書型変数を print() で表示する (見やすくするためにjson.dumpsを使用)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ 収集結果の辞書型変数 (URL: <title>の内容)\")\n",
    "print(\"=\"*50)\n",
    "# print() 関数で表示\n",
    "print(json.dumps(page_data, indent=4, ensure_ascii=False))\n",
    "# ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
